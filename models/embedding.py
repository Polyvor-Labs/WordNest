import torch
import torch.nn as nn
import numpy as np

class RotaryEmbedding(nn.Module):
    """
    RotaryEmbedding generates rotary positional embeddings for use in attention mechanisms.
    It computes inverse frequencies based on the specified dimension, which are later used
    to generate sinusoidal embeddings for each token position.
    
    Attributes:
        inv_freq (torch.Tensor): Precomputed inverse frequency tensor for rotary embeddings.
    """
    def __init__(self, dim):
        """
        Initializes the RotaryEmbedding module by computing and storing inverse frequency values.

        Args:
            dim (int): The dimensionality for which to compute the rotary embeddings.
                       Typically, this should match the head dimension of the model.
        """
        super().__init__()
        # Calculate inverse frequencies for even indices in the dimension.
        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))
        # Register the inverse frequency tensor as a buffer so it is not updated during training.
        self.register_buffer("inv_freq", inv_freq)

    def forward(self, seq_len):
        """
        Generates rotary positional embeddings for a given sequence length.

        Args:
            seq_len (int): The sequence length (number of tokens) for which to generate embeddings.

        Returns:
            torch.Tensor: A tensor of shape (seq_len, dim) containing rotary positional embeddings.
                          The embeddings are generated by computing the outer product of token positions
                          and the precomputed inverse frequencies, then duplicating for sin and cos components.
        """
        # Create a tensor containing positions [0, 1, ..., seq_len-1]
        t = torch.arange(seq_len, device=self.inv_freq.device).type_as(self.inv_freq)
        # Compute the outer product to get frequency values for each position.
        freqs = torch.einsum("i,j->ij", t, self.inv_freq)
        # Duplicate the frequencies to cover the full dimension (sin and cos parts)
        return torch.cat((freqs, freqs), dim=-1)

def rotate_half(x):
    """
    Rotates half of the dimensions of the input tensor.
    
    This helper function splits the last dimension of the tensor into two halves.
    It then negates the second half and concatenates it with the first half,
    effectively performing a rotation in the embedding space.

    Args:
        x (torch.Tensor): Input tensor whose last dimension is divisible by 2.

    Returns:
        torch.Tensor: The tensor after applying the half rotation.
    """
    # Split the tensor into two halves along the last dimension.
    x1, x2 = x.chunk(2, dim=-1)
    # Concatenate the negated second half with the first half.
    return torch.cat((-x2, x1), dim=-1)

def apply_rope(x, freqs):
    """
    Applies rotary positional embeddings to the input tensor.
    
    This function modifies the input tensor by combining it with sinusoidal functions
    derived from the positional frequency embeddings. The formula applied is:
    
        output = x * cos(freqs) + rotate_half(x) * sin(freqs)
    
    Args:
        x (torch.Tensor): Input tensor with shape (..., dim).
        freqs (torch.Tensor): Positional frequency tensor with the same shape as x.

    Returns:
        torch.Tensor: The tensor after applying rotary positional embedding transformations.
    """
    return (x * freqs.cos()) + (rotate_half(x) * freqs.sin())
